# app.py ‚Äî Credit Scoring (Streamlit) ‚Äî FAST TRAIN edition
# - Gi·∫£m k√≠ch th∆∞·ªõc t·∫≠p train (stratified sample)
# - RF c√≥ th√™m tham s·ªë: n_estimators, max_depth, max_features, min_samples_leaf
# - SHAP t·∫Øt m·∫∑c ƒë·ªãnh, khi b·∫≠t v·∫´n gi·ªõi h·∫°n m·∫´u + √©p float64
# - Gi·ªØ m·ªçi fix tr∆∞·ªõc: subset c·ªôt, pickling no-lambda, ∆∞u ti√™n model ƒë√£ fit, CM, v.v.

import os
from pathlib import Path
import numpy as np
import pandas as pd
import joblib
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix
from sklearn.exceptions import NotFittedError
from sklearn.utils.validation import check_is_fitted

import shap
import matplotlib.pyplot as plt

# ============== C·∫§U H√åNH ==============
FEATURE_SUBSET = [
    "Age","Annual_Income","Monthly_Inhand_Salary","Num_Bank_Accounts","Num_Credit_Card",
    "Interest_Rate","Num_of_Loan","Occupation","Type_of_Loan","Delay_from_due_date",
]
MODEL_PATHS = [
    "models/random_forest.pkl","models/logistic_regression.pkl","models/uploaded.pkl","models/xgboost.pkl"
]

# ============== H√ÄM H·ªñ TR·ª¢ ==============
def to_str(X): return X.astype(str)

@st.cache_data(show_spinner=False)
def read_csv_cached(file): return pd.read_csv(file)

def is_fitted_estimator(est)->bool:
    try: check_is_fitted(est); return True
    except Exception: pass
    for a in ("classes_","n_features_in_"):
        if hasattr(est,a): return True
    return False

def load_any_model():
    for p in MODEL_PATHS:
        if os.path.exists(p):
            try:
                mdl = joblib.load(p)
                if isinstance(mdl,Pipeline) and "clf" in mdl.named_steps:
                    if not is_fitted_estimator(mdl.named_steps["clf"]): continue
                else:
                    if not is_fitted_estimator(mdl): continue
                return mdl,p
            except Exception as e:
                st.warning(f"L·ªói n·∫°p model {p}: {e}")
    return None,None

def save_model(model,out_path:str):
    Path("models").mkdir(parents=True,exist_ok=True)
    joblib.dump(model,out_path); return out_path

def split_features_target(df,target_col):
    return df.drop(columns=[target_col]), df[target_col]

def infer_cols(X):
    num=[c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat=[c for c in X.columns if c not in num]
    return num,cat

def build_pipeline(model_type, X_sample, rf_params:dict|None=None)->Pipeline:
    rf_params = rf_params or {}
    num_cols, cat_cols = infer_cols(X_sample)
    num_tf = Pipeline([("imputer",SimpleImputer(strategy="median"))])
    cat_tf = Pipeline([
        ("imputer",SimpleImputer(strategy="most_frequent")),
        ("to_str",FunctionTransformer(to_str)),    # tr√°nh lambda
        ("ohe",OneHotEncoder(handle_unknown="ignore")),
    ])
    pre = ColumnTransformer([("num",num_tf,num_cols),("cat",cat_tf,cat_cols)])
    if model_type=="Logistic Regression":
        clf = LogisticRegression(max_iter=200)
    else:
        clf = RandomForestClassifier(
            n_estimators=rf_params.get("n_estimators",100),
            max_depth=rf_params.get("max_depth",None),
            max_features=rf_params.get("max_features","sqrt"),
            min_samples_leaf=rf_params.get("min_samples_leaf",1),
            random_state=42, n_jobs=-1
        )
    return Pipeline([("pre",pre),("clf",clf)])

def get_feature_names_from_pre(pre:ColumnTransformer)->list[str]:
    names=[]
    for name,trans,cols in pre.transformers_:
        if name=='remainder' and trans=='drop': continue
        if hasattr(trans,'named_steps'):
            last=list(trans.named_steps.values())[-1]
            if hasattr(last,'get_feature_names_out'):
                base=cols if isinstance(cols,list) else [cols]
                try: names+=last.get_feature_names_out(base).tolist()
                except Exception: names+= base
            else: names+= cols if isinstance(cols,list) else [cols]
        else: names+= cols if isinstance(cols,list) else [cols]
    return names

def ensure_subset_and_types(df):
    X=df.copy()
    for c in FEATURE_SUBSET:
        if c not in X.columns: X[c]=pd.NA
    X=X[FEATURE_SUBSET]
    for c in FEATURE_SUBSET:
        conv=pd.to_numeric(X[c],errors="coerce")
        X[c]=conv if conv.notna().any() else X[c].astype(str)
    return X

def densify_float64(X):
    if hasattr(X,"toarray"): X=X.toarray()
    return np.asarray(X,dtype=np.float64)

def try_predict(model,X_df):
    try:
        X_df=ensure_subset_and_types(X_df)
        if isinstance(model,Pipeline):
            pre,clf = model.named_steps["pre"], model.named_steps["clf"]
            X_tr=pre.transform(X_df)
            pred=clf.predict(X_tr)[0]
            proba=classes=None
            if hasattr(clf,"predict_proba"):
                pp=clf.predict_proba(X_tr)[0]; classes=getattr(clf,"classes_",[str(i) for i in range(len(pp))]); proba=pp
            return pred,proba,classes,X_tr,get_feature_names_from_pre(pre)
        else:
            pred=model.predict(X_df)[0]
            proba=classes=None
            if hasattr(model,"predict_proba"):
                pp=model.predict_proba(X_df)[0]; classes=getattr(model,"classes_",[str(i) for i in range(len(pp))]); proba=pp
            return pred,proba,classes,X_df.to_numpy(),list(X_df.columns)
    except NotFittedError:
        return "L·ªói: M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán.",None,None,None,None
    except Exception as e:
        return f"L·ªói d·ª± ƒëo√°n: {e}",None,None,None,None

def plot_confusion_matrix_cm(y_true,y_pred,labels=None):
    cm=confusion_matrix(y_true,y_pred,labels=labels)
    fig,ax=plt.subplots(); im=ax.imshow(cm,interpolation='nearest'); ax.figure.colorbar(im,ax=ax)
    ax.set_title("Confusion Matrix"); ax.set_xlabel("Predicted"); ax.set_ylabel("True")
    ticks=np.arange(len(labels)) if labels is not None else np.arange(cm.shape[0])
    ax.set_xticks(ticks); ax.set_yticks(ticks)
    ax.set_xticklabels(labels if labels is not None else ticks,rotation=45,ha="right")
    ax.set_yticklabels(labels if labels is not None else ticks)
    thresh=cm.max()/2.0 if cm.size else .5
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j,i,format(cm[i,j],"d"),ha="center",va="center",color="white" if cm[i,j]>thresh else "black")
    fig.tight_layout(); st.pyplot(fig)

def try_build_tree_explainer(pipe:Pipeline,X_bg):
    try:
        pre,clf = pipe.named_steps["pre"], pipe.named_steps["clf"]
        X_bg_tr = densify_float64(pre.transform(ensure_subset_and_types(X_bg)))
        if isinstance(clf,RandomForestClassifier):
            return shap.TreeExplainer(clf,feature_names=get_feature_names_from_pre(pre)), get_feature_names_from_pre(pre), X_bg_tr
    except Exception as e:
        st.info(f"Kh√¥ng t·∫°o ƒë∆∞·ª£c TreeExplainer: {e}")
    return None,None,None

def global_shap_bar(explainer,X_tr,feature_names,topk=15):
    X_tr=densify_float64(X_tr)
    shap_vals=explainer.shap_values(X_tr)
    if isinstance(shap_vals,list):
        abs_means=np.stack([np.mean(np.abs(sv),axis=0) for sv in shap_vals],axis=0); imp=abs_means.max(axis=0)
    else:
        imp=np.mean(np.abs(shap_vals),axis=0)
    idx=np.argsort(imp)[::-1][:topk]
    fig,ax=plt.subplots(); ax.barh([feature_names[i] for i in idx][::-1],imp[idx][::-1])
    ax.set_xlabel("|SHAP value| (mean)"); ax.set_title("Global feature importance (SHAP)")
    fig.tight_layout(); st.pyplot(fig)

def local_shap_table(explainer,x1_tr,feature_names,topk=10):
    x1=densify_float64(x1_tr).reshape(1,-1)
    shap_vals=explainer.shap_values(x1)
    if isinstance(shap_vals,list):
        totals=[np.sum(np.abs(sv)) for sv in shap_vals]; sv=shap_vals[int(np.argmax(totals))][0]
    else: sv=shap_vals[0]
    order=np.argsort(np.abs(sv))[::-1][:topk]
    st.subheader("ƒê√≥ng g√≥p theo ƒë·∫∑c tr∆∞ng (SHAP) ‚Äì b·∫£n ghi hi·ªán t·∫°i")
    st.dataframe(pd.DataFrame({"feature":[feature_names[i] for i in order],
                               "shap_value":sv[order],
                               "contribution":np.where(sv[order]>=0,"‚Üë tƒÉng r·ªßi ro","‚Üì gi·∫£m r·ªßi ro")}),
                 use_container_width=True)

def fake_lookup(identifier:str)->pd.DataFrame:
    return pd.DataFrame([{
        "Age":30,"Annual_Income":30000,"Monthly_Inhand_Salary":2000,"Num_Bank_Accounts":2,
        "Num_Credit_Card":1,"Interest_Rate":12,"Num_of_Loan":1,"Occupation":"Employee",
        "Type_of_Loan":"Personal","Delay_from_due_date":0
    }])

def ensure_frame(x):
    return pd.DataFrame([x]) if isinstance(x,dict) else x

# ============== SIDEBAR ==============
st.sidebar.header("Ch·∫ø ƒë·ªô")
mode=st.sidebar.radio("Ch·ªçn ch·∫ø ƒë·ªô",["Tra nhanh (SƒêT/CCCD/ID)","Ch·∫•m ƒëi·ªÉm chi ti·∫øt","Gi·∫£i th√≠ch & ƒê·∫°o ƒë·ª©c","Qu·∫£n tr·ªã (train & upload)"])
model,model_path=load_any_model()
if "explainer" not in st.session_state: st.session_state.explainer=None
if "feature_names" not in st.session_state: st.session_state.feature_names=None
st.sidebar.success(f"Model: {os.path.basename(model_path)}" if model_path else "Ch∆∞a c√≥ model ‚Äì m√¥ ph·ªèng")

# ============== 1) TRA NHANH ==============
if mode=="Tra nhanh (SƒêT/CCCD/ID)":
    st.title("Tra nhanh ƒëi·ªÉm t√≠n d·ª•ng")
    st.write("- Dataset Kaggle th∆∞·ªùng **kh√¥ng c√≥ SƒêT** ‚Üí h√£y nh·∫≠p **ID/Customer_ID**. N·∫øu kh√¥ng upload CSV, h·ªá th·ªëng d√πng demo gi·∫£ l·∫≠p.")
    identifier=st.text_input("Nh·∫≠p SƒêT/CCCD/ID:")
    st.markdown("**(Tu·ª≥ ch·ªçn)** T·∫£i dataset CSV ƒë·ªÉ tra c·ª©u b·∫£n ghi th·∫≠t:")
    search_csv=st.file_uploader("Upload CSV (c√≥ c·ªôt ID/SƒêT/CCCD/Customer_ID)",type=["csv"],key="search_csv")
    id_column=df_search=None
    if search_csv is not None:
        df_search=read_csv_cached(search_csv)
        st.dataframe(df_search.head())
        guess=[c for c in df_search.columns if c.lower() in ("id","customer_id","phone","cccd","sdt")]
        id_column=st.selectbox("Ch·ªçn c·ªôt ƒë·ªãnh danh ƒë·ªÉ tra",options=df_search.columns,
                               index=0 if not guess else df_search.columns.get_loc(guess[0]))
    if st.button("Ki·ªÉm tra"):
        if not identifier.strip():
            st.warning("Vui l√≤ng nh·∫≠p ID/Customer_ID.")
        else:
            if df_search is not None and id_column:
                row=df_search[df_search[id_column].astype(str)==str(identifier)].drop(columns=[id_column],errors="ignore")
                if row.empty:
                    st.error("Kh√¥ng t√¨m th·∫•y h·ªì s∆° trong CSV."); st.stop()
                X=row
            else:
                X=ensure_frame(fake_lookup(identifier))
            if model is None:
                X=ensure_subset_and_types(X)
                s=(X.get("Annual_Income",pd.Series([30000])).iloc[0]/10000 +
                   X.get("Monthly_Inhand_Salary",pd.Series([2000])).iloc[0]/1000 -
                   X.get("Num_of_Loan",pd.Series([1])).iloc[0]*0.5 -
                   X.get("Delay_from_due_date",pd.Series([0])).iloc[0]/20 -
                   X.get("Interest_Rate",pd.Series([12])).iloc[0]/30)
                st.info(f"K·∫øt qu·∫£ (m√¥ ph·ªèng): **{'T·ªët' if s>=1.5 else ('Trung b√¨nh' if s>=0.9 else 'K√©m')}**")
            else:
                pred,proba,classes,X_tr,feat_names=try_predict(model,X)
                if isinstance(pred,str) and pred.startswith("L·ªói"): st.error(pred)
                else:
                    st.success(f"K·∫øt qu·∫£: **{pred}**")
                    if proba is not None and classes is not None:
                        st.subheader("X√°c su·∫•t"); [st.write(f"- {c}: {p:.2%}") for c,p in zip(classes,proba)]
                    if st.session_state.explainer is not None and X_tr is not None:
                        with st.expander("Gi·∫£i th√≠ch (SHAP) ‚Äì b·∫£n ghi hi·ªán t·∫°i"):
                            local_shap_table(st.session_state.explainer,X_tr,st.session_state.feature_names,topk=10)
                    else: st.caption("G·ª£i √Ω: Train Random Forest ·ªü tab Qu·∫£n tr·ªã ƒë·ªÉ b·∫≠t SHAP.")

# ============== 2) CH·∫§M ƒêI·ªÇM CHI TI·∫æT ==============
elif mode=="Ch·∫•m ƒëi·ªÉm chi ti·∫øt":
    st.title("Ch·∫•m ƒëi·ªÉm t·ª´ ti√™u ch√≠ chi ti·∫øt")
    with st.form("credit_form"):
        age=st.number_input("Tu·ªïi",18,100,30)
        income=st.number_input("Annual Income",1000,10_000_000,30000)
        inhand=st.number_input("Monthly Inhand Salary",100,100_000,2000)
        num_acc=st.number_input("S·ªë t√†i kho·∫£n ng√¢n h√†ng",0,50,2)
        num_card=st.number_input("S·ªë th·∫ª t√≠n d·ª•ng",0,50,1)
        rate=st.number_input("L√£i su·∫•t (%)",0,100,12)
        num_loan=st.number_input("S·ªë kho·∫£n vay",0,50,1)
        occupation=st.text_input("Ngh·ªÅ nghi·ªáp","Employee")
        loan_type=st.text_input("Lo·∫°i kho·∫£n vay","Personal")
        delay=st.number_input("S·ªë ng√†y tr·ªÖ h·∫°n",0,1000,0)
        submitted=st.form_submit_button("Ch·∫•m ƒëi·ªÉm")
    if submitted:
        X=ensure_frame({"Age":age,"Annual_Income":income,"Monthly_Inhand_Salary":inhand,"Num_Bank_Accounts":num_acc,
                        "Num_Credit_Card":num_card,"Interest_Rate":rate,"Num_of_Loan":num_loan,
                        "Occupation":occupation,"Type_of_Loan":loan_type,"Delay_from_due_date":delay})
        if model is None:
            X=ensure_subset_and_types(X)
            s=income/10000 + inhand/1000 - num_loan*0.5 - delay/20 - rate/30
            st.info(f"K·∫øt qu·∫£ (m√¥ ph·ªèng): **{'T·ªët' if s>=1.5 else ('Trung b√¨nh' if s>=0.9 else 'K√©m')}**")
        else:
            pred,proba,classes,X_tr,feat_names=try_predict(model,X)
            if isinstance(pred,str) and pred.startswith("L·ªói"): st.error(pred)
            else:
                st.success(f"K·∫øt qu·∫£: **{pred}**")
                if proba is not None and classes is not None:
                    st.subheader("X√°c su·∫•t"); [st.write(f"- {c}: {p:.2%}") for c,p in zip(classes,proba)]
                if st.session_state.explainer is not None and X_tr is not None:
                    with st.expander("Gi·∫£i th√≠ch (SHAP) ‚Äì b·∫£n ghi hi·ªán t·∫°i"):
                        local_shap_table(st.session_state.explainer,X_tr,st.session_state.feature_names,topk=10)
                else: st.caption("G·ª£i √Ω: Train Random Forest ·ªü tab Qu·∫£n tr·ªã ƒë·ªÉ b·∫≠t SHAP.")

# ============== 3) GI·∫¢I TH√çCH & ƒê·∫†O ƒê·ª®C ==============
elif mode=="Gi·∫£i th√≠ch & ƒê·∫°o ƒë·ª©c":
    st.title("Gi·∫£i th√≠ch & ƒê·∫°o ƒë·ª©c")
    st.markdown("""
**Minh b·∫°ch**: cung c·∫•p gi·∫£i th√≠ch c·ª•c b·ªô (SHAP) cho t·ª´ng d·ª± ƒëo√°n; quy tr√¨nh khi·∫øu n·∫°i/so√°t x√©t.  
**Quy·ªÅn ri√™ng t∆∞**: ·∫©n danh ƒë·ªãnh danh; t√°ch PII; h·∫°n ch·∫ø truy c·∫≠p.  
**C√¥ng b·∫±ng**: theo d√µi bias theo nh√≥m; c√¢n b·∫±ng d·ªØ li·ªáu; ƒëi·ªÅu ch·ªânh ng∆∞·ª°ng.  
**B·∫£o m·∫≠t**: t√°ch d·ªãch v·ª• scoring; ki·ªÉm so√°t truy c·∫≠p; ghi log ·∫©n danh.
    """)

# ============== 4) QU·∫¢N TR·ªä (TRAIN & UPLOAD) ==============
else:
    st.title("Qu·∫£n tr·ªã (train & upload)")
    st.subheader("A) Upload model c√≥ s·∫µn (.pkl)")
    f=st.file_uploader("Upload model (.pkl)",type=["pkl"])
    if f:
        out=Path("models/uploaded.pkl"); out.parent.mkdir(parents=True,exist_ok=True); out.write_bytes(f.read())
        st.success(f"ƒê√£ l∆∞u model: {out}")
        try:
            new_model,new_path=load_any_model()
            if new_model: st.info(f"Model ƒëang d√πng: {new_path}")
        except Exception as e: st.error(f"L·ªói n·∫°p model upload: {e}")

    st.markdown("---")
    st.subheader("B) Train nhanh t·ª´ CSV (Kaggle/ngu·ªìn kh√°c)")
    up=st.file_uploader("Upload CSV Train",type=["csv"],key="train_csv")
    if up is not None:
        df_train=read_csv_cached(up)
        st.write("Xem tr∆∞·ªõc d·ªØ li·ªáu:"); st.dataframe(df_train.head())

        # ch·ªçn target & ID
        guess_t=next((c for c in ["Credit_Score","risk_flag","label","target"] if c in df_train.columns),None)
        target_col=st.selectbox("Ch·ªçn c·ªôt nh√£n (target)",options=df_train.columns,
                                index=df_train.columns.get_loc(guess_t) if guess_t else 0)
        guess_id=next((c for c in ["ID","Customer_ID","customer_id"] if c in df_train.columns),None)
        id_col=st.selectbox("Ch·ªçn c·ªôt ID (tu·ª≥ ch·ªçn, s·∫Ω b·ªè khi train)",
                            options=["(Kh√¥ng d√πng)"]+list(df_train.columns),
                            index=(["(Kh√¥ng d√πng)"]+list(df_train.columns)).index(guess_id) if guess_id else 0)

        df_work=df_train.drop(columns=[id_col]) if id_col!="(Kh√¥ng d√πng)" else df_train.copy()

        missing=[c for c in FEATURE_SUBSET if c not in df_work.columns]
        if missing:
            st.error("Dataset thi·∫øu c√°c c·ªôt cho demo (kh·ªõp form): "+", ".join(missing)); st.stop()

        df_model=df_work[FEATURE_SUBSET+[target_col]].copy()
        for c in FEATURE_SUBSET:
            conv=pd.to_numeric(df_model[c],errors="coerce")
            df_model[c]=conv if conv.notna().any() else df_model[c].astype(str)

        # Gi·∫£m k√≠ch th∆∞·ªõc t·∫≠p train
        reduce_frac = st.selectbox("Gi·∫£m k√≠ch th∆∞·ªõc t·∫≠p train (ƒë·ªÉ nhanh)", ["1.0 (kh√¥ng gi·∫£m)","0.5","0.25","0.1"], index=0)
        reduce_frac = float(reduce_frac.split()[0])

        X_all,y_all=split_features_target(df_model,target_col)
        if reduce_frac<1.0:
            # stratified sample
            tmp=X_all.copy(); tmp[target_col]=y_all.values
            X_all = tmp.groupby(target_col, group_keys=False).apply(lambda d: d.sample(max(1,int(len(d)*reduce_frac)), random_state=42)).drop(columns=[target_col])
            y_all = tmp.groupby(target_col, group_keys=False).apply(lambda d: d.sample(max(1,int(len(d)*reduce_frac)), random_state=42))[target_col]

        test_size=st.slider("T·ªâ l·ªá test",0.1,0.4,0.2,step=0.05)
        model_type=st.selectbox("Ch·ªçn thu·∫≠t to√°n",["Logistic Regression","Random Forest"])

        # Tham s·ªë RF ƒë·ªÉ nhanh
        rf_params={}
        enable_shap=False
        if model_type=="Random Forest":
            rf_params["n_estimators"]=st.slider("S·ªë c√¢y Random Forest",50,400,100,step=50)
            md = st.selectbox("max_depth",["None","5","10","20"],index=0)
            rf_params["max_depth"]=None if md=="None" else int(md)
            mf = st.selectbox("max_features",["sqrt","log2","0.5"],index=0)
            rf_params["max_features"]=0.5 if mf=="0.5" else mf
            rf_params["min_samples_leaf"]=st.selectbox("min_samples_leaf",[1,2,4,8],index=1)
            enable_shap = st.checkbox("B·∫≠t SHAP (c√≥ th·ªÉ ch·∫≠m)", value=False)

        if st.button("Hu·∫•n luy·ªán"):
            try:
                vc=y_all.value_counts(); strat=None if (vc<2).any() else y_all
                X_tr,X_te,y_tr,y_te=train_test_split(X_all,y_all,test_size=test_size,random_state=42,stratify=strat)
                pipe=build_pipeline(model_type,X_tr,rf_params=rf_params)

                with st.status("ƒêang hu·∫•n luy·ªán...",expanded=True) as status:
                    pipe.fit(X_tr,y_tr); st.write("‚úÖ ƒê√£ fit xong model.")
                    y_pred=pipe.predict(X_te)
                    acc=accuracy_score(y_te,y_pred); f1=f1_score(y_te,y_pred,average="weighted")
                    st.write("**K·∫øt qu·∫£ tr√™n t·∫≠p test:**"); st.write(f"- Accuracy: {acc:.4f}"); st.write(f"- F1 (weighted): {f1:.4f}")
                    st.code(classification_report(y_te,y_pred),language="text")
                    with st.expander("Confusion Matrix"):
                        labels=[str(c) for c in sorted(pd.unique(pd.concat([y_tr,y_te])))]
                        plot_confusion_matrix_cm(y_te,y_pred,labels=labels)

                    # SHAP (gi·ªõi h·∫°n m·∫´u)
                    st.session_state.explainer=None; st.session_state.feature_names=None
                    if model_type=="Random Forest" and enable_shap:
                        st.write("üîé ƒêang t·∫°o SHAP (gi·ªõi h·∫°n m·∫´u)‚Ä¶")
                        bg_n=min(150,len(X_tr)); te_n=min(200,len(X_te))
                        X_bg=X_tr.sample(bg_n,random_state=42) if len(X_tr)>bg_n else X_tr
                        X_te_small=X_te.sample(te_n,random_state=42) if len(X_te)>te_n else X_te
                        explainer,feat_names,_=try_build_tree_explainer(pipe,X_bg)
                        if explainer is not None:
                            st.session_state.explainer=explainer; st.session_state.feature_names=feat_names
                            with st.expander(f"Global Feature Importance (SHAP) ‚Äî {te_n} m·∫´u"):
                                pre=pipe.named_steps["pre"]
                                X_te_tr=densify_float64(pre.transform(ensure_subset_and_types(X_te_small)))
                                global_shap_bar(explainer,X_te_tr,feat_names,topk=15)
                        else: st.info("Kh√¥ng t·∫°o ƒë∆∞·ª£c SHAP Explainer.")
                    status.update(label="Hu·∫•n luy·ªán xong!",state="complete")

                out_name="models/logistic_regression.pkl" if model_type=="Logistic Regression" else "models/random_forest.pkl"
                save_model(pipe,out_name); st.success(f"üíæ ƒê√£ l∆∞u model: {out_name}")

            except Exception as e:
                st.error(f"L·ªói train: {e}")
